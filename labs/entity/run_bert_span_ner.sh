CUDA_VISIBLE_DEVICES=0 \
python train_bert_span_ner.py \
    --pretrain_path /home/liujian/NLP/corpus/transformers/hfl-chinese-bert-wwm-ext \
    --bert_name bert \
    --model single \
    --metric micro_f1 \
    --dataset policy \
    --compress_seq \
    --tagscheme bmoes \
    --batch_size 1024 \
    --lr 1e-3 \
    --bert_lr 1e-5 \
    --weight_decay 0 \
    --warmup_step 0 \
    --max_length 256 \
    --max_epoch 16 \
    --ffn_hidden_size 150 \
    --width_embedding_size 150 \
    --dropout_rate 0.1 \
    --max_span 7 \
    --soft_label True \
    --loss ce \
    --adv fgm \
    --optimizer adamw
